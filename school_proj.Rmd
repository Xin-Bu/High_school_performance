---
title: "multi_linear_regression_bootstrap"
author: "Xin Bu"
date: "11/14/2023"
output: html_document
---

```{r setup, include=FALSE}
library(tidyverse,warn.conflicts = F)
library(tidymodels,warn.conflicts = F)
library(lubridate,warn.conflicts = F)
library(here)
library(table1)
library(kknn)
library(kernlab)

knitr::opts_chunk$set(echo = TRUE)

```

## R Markdown

datasource:kaggle


```{r model-fit, message=F}
d <- read_csv("high_school.csv")

```

```{r message=F,warning=F}

d <- d %>% 
    rename("Gender" = "gender", "Eth" = "race/ethnicity", "Par_edu" = "parental level of education", "Lunch" = "lunch", "Test_prep" = "test preparation course", "Math" = "math score", "Reading" = "reading score", "Writing" = "writing score")

if(F){
  skimr::skim(d)
  Hmisc::describe(d)
}
```


```{r model-fit3, message=F}
ggplot(data = d) +
  geom_point(mapping = aes(x = Math, y = Reading, color = Lunch)) +
  facet_wrap(~Par_edu) + 
  scale_color_hue(h=c(45, 245))
```

```{r}
ggplot(data = d, mapping = aes(x = Eth, fill = Par_edu)) + 
  geom_bar(alpha = 1/5, position = "identity")

```


```{r}
ggplot(data = d) +
  geom_boxplot(mapping = aes(x = reorder(Par_edu, Writing, FUN = median), y = Writing, color=Gender)) +
  facet_wrap(~Lunch)+ 
  scale_color_hue(h=c(45, 245)) +
  coord_flip()
```

```{r}
table1::label(d$Gender) <= "Gender"
table1::label(d$Eth)<="Race/Ethnicity"
table1::label(d$Lunch) <="Lunch"
table1::label(d$Par_edu)<="Parental Level of Education"
table1::label(d$Test_prep) <= "Test Preparation"
table1::label(d$Math)<= "Math"
table1::label(d$Reading) <= "Reading"
table1::label(d$Writing) <= "Writing"

table1::table1(~Math+Reading+Writing|Eth, data=d)
table1::table1(~Math+Reading+Writing|Lunch, data=d)
table1::table1(~Math+Reading+Writing|Gender,data=d)
table1::table1(~Math+Reading+Writing|Test_prep, data=d)
table1::table1(~Math+Reading+Writing|Par_edu, data = d)

```



```{r model-fit2, message=F}


lm.1 <- lm(Math ~ Eth + Lunch + Par_edu + Test_prep + Gender, data=d)

mod.summary <- summary(lm.1)

# could also use `coef(lm.1) for coefficients, but can't get SEs that way
coefs <- mod.summary$coefficients[,1]
coef.se <- mod.summary$coefficients[,2] 
rse <- mod.summary$sigma


for.table <- tibble(term=rownames(mod.summary$coefficients),
                    coefficient=coefs,
                    se=coef.se)

knitr::kable(for.table,digits=2)

```

```{r}
set.seed(20231116)

# do splits, make recipe. 

splits <- initial_split(d,strata=Par_edu)
cv_folds <- vfold_cv(training(splits))

high_school.rec <- 
  recipe(Math ~ Eth + Lunch + Gender + Test_prep,
         data=training(splits)) %>% 
  step_dummy(all_nominal()) %>% 
  step_normalize(all_predictors()) %>% 
  prep()

```


```{r}
high_school.linear <- linear_reg() %>% 
  set_engine("lm")

# SVM and KNN + grids

high_school.svm <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune()) %>% 
  set_engine("kernlab") %>% 
  set_mode("regression")

high_school.knn <- nearest_neighbor(
  neighbors = tune()) %>% 
  set_engine("kknn") %>% 
  set_mode("regression")

lin.wf <- workflow() %>% 
  add_recipe(high_school.rec) %>% 
  add_model(high_school.linear)

svm.wf <- workflow() %>% 
  add_recipe(high_school.rec) %>% 
  add_model(high_school.svm)

knn.wf <- workflow() %>% 
  add_recipe(high_school.rec) %>% 
  add_model(high_school.knn)
```


```{r cache = T}

knn.grid <- grid_regular(
  neighbors(range = c(1,50)),
  levels=10
)

svm.grid <- grid_regular(
  cost(),
  rbf_sigma(),
  levels=4)

  
# tune_grid

knn.results <- 
  knn.wf %>% 
  tune_grid(
    resamples=cv_folds,
    grid=knn.grid)

svm.results <- 
  svm.wf %>% 
  tune_grid(
    resamples=cv_folds,
    grid=svm.grid)


if(F){
  knn.results %>% collect_metrics() %>% filter(.metric=="rsq") %>% arrange(mean)
  svm.results %>% collect_metrics() %>% filter(.metric=="rmse") %>% arrange(mean)
}

```


```{r}
# finalize workflows and do fits
svm.final <- svm.wf %>% 
  finalize_workflow(svm.results %>% select_best("rmse")) %>% 
  fit(training(splits))

knn.final <- knn.wf %>% 
  finalize_workflow(knn.results %>% select_best("rmse")) %>% 
  fit(training(splits))

lin.final <- lin.wf %>% 
  fit(training(splits))


high_school <- testing(splits)

high_school <- high_school %>% 
  mutate(svm_pred = predict(svm.final,new_data=high_school) %>% pull(.pred)) %>% 
  mutate(knn_pred = predict(knn.final,new_data=high_school) %>% pull(.pred)) %>% 
  mutate(lin_pred = predict(lin.final,new_data=high_school) %>% pull(.pred)) 

holder <- metrics(high_school,truth=Math,estimate=svm_pred) %>% 
  select(-.estimator) %>% 
  rename(metric = .metric,
         svm = .estimate)

holder <- holder %>% 
  mutate(knn = 
    metrics(high_school,truth=Math,estimate=knn_pred) %>% 
      pull(.estimate)
  ) %>% 
  mutate(linear = 
    metrics(high_school,truth=Math,estimate=lin_pred) %>% 
      pull(.estimate)
  )

holder %>% 
  pivot_longer(-metric) %>% 
  ggplot(aes(x=name,y=value,group=1)) + 
  geom_point() + 
  facet_wrap(~metric,scales="free_y") + 
  theme_bw() +
  labs(x="Model",y="Metric")


```
## Model evaluation
Now, we are going to use bootstrap to evaluate the model. 

```{r model-boot, message=F}

# Using the base method
n.sim <- 1000
results <- tibble(intercept = rep(0,n.sim),
                  Eth1 = 0, 
                  Eth2 = 0,
                  Eth3 = 0,
                  Eth4 = 0,
                  Lunch = 0, 
                  Par_edu1 = 0, 
                  Par_edu2 = 0,
                  Par_edu3 = 0,
                  Par_edu4 = 0,
                  Par_edu5 = 0,
                  Test_prep = 0, 
                  Gender = 0, 
                  r2 = 0, 
                  rse = 0)


for(i in 1:n.sim) {
  new.d <- d %>% 
    slice_sample(n=nrow(d),replace = T)

  this.lm <- lm(Math ~ Eth + Lunch + Par_edu + Test_prep + Gender, data=new.d)
  
  # just do all the stats in one row. 
  results$intercept[i] <- coef(this.lm)[1]
  results$Eth1[i] <- coef(this.lm)[2]
  results$Eth2[i] <- coef(this.lm)[3]
  results$Eth3[i] <- coef(this.lm)[4]
  results$Eth4[i] <- coef(this.lm)[5]
  results$Lunch[i] <- coef(this.lm)[6]
  results$Par_edu1[i] <- coef(this.lm)[7]
  results$Par_edu2[i] <- coef(this.lm)[8]
  results$Par_edu3[i] <- coef(this.lm)[9]
  results$Par_edu4[i] <- coef(this.lm)[10]
  results$Par_edu5[i] <- coef(this.lm)[11]
  results$Test_prep[i] <- coef(this.lm)[12]
  results$Gender[i] <- coef(this.lm)[13]
  results$r2[i] <- summary(this.lm)$adj.r.squared
  results$rse[i] <- summary(this.lm)$sigma
  
}


for.table <- tibble(term=c(rownames(mod.summary$coefficients),"adj.r.sq","rse"),
                    coefficient=apply(results,2,mean),
                    se=apply(results,2,sd),
                    lower_bound=apply(results,2,quantile,prob=0.05),
                    upper_bound=apply(results,2,quantile,prob=0.95))

knitr::kable(for.table,digits=2)

```

```

```{r}


```

```{r}


```